import os, json
from pathlib import Path
from datetime import datetime, timezone
from airflow.decorators import dag, task
from airflow.operators.postgres_operator import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timezone
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import pandas as pd

AWS_S3_CONN_ID = "S3_conn"
DB_CONN = "postgres_conn"
S3_BUCKNAME = 'ymoon-au-dbt-fx-raw'
JOB_NAME = "stock_financials" 
TARGET_PATH = "hist/%s" % datetime.now(timezone.utc).strftime("%Y%m")
LOOKUP_PATH = "lookups"
FNAME_EXT = "json"
STLIST_FNAME = "NZX-50.csv"

def _get_stock_symbols():
    filepath = '/tmp/%s/%s/%s' % (S3_BUCKNAME, LOOKUP_PATH, STLIST_FNAME)
    df = pd.read_csv(filepath, usecols=['Profile'])
    return df

def _get_financials_file(symbol):
    filepath = '/tmp/%s/%s/%s-%s.%s' % (S3_BUCKNAME, TARGET_PATH, JOB_NAME, symbol, FNAME_EXT)
    with open(filepath) as file:
        jsonv = json.load(file)
    return jsonv

@dag(start_date=datetime(2024, 11, 11), schedule='@monthly', catchup=False)
def nzx50_financials_process2_db():

    @task()
    def check4_previous_run():
        print("Checking database for financials If already Run...")
        # Establish connection using PostgresHook
        hook = PostgresHook(postgres_conn_id=DB_CONN)
        
        # Run a query to fetch the first row
        sql_query = """SELECT * FROM st.run_log 
            where log_type ='%s' and log_info='%s:%s' and log_status = 'COMPLETED'
            LIMIT 1""" % (JOB_NAME, TARGET_PATH, STLIST_FNAME)    
        result = hook.get_first(sql_query)
        
        # Log and return the result
        if result:
            print(f"Previous Run: {result}")
            raise Exception("Already Completed Previously!")
            return result
        else:
            print("No rows found in the table.")
            return None

    @task
    def getfrom_S3():
        print ('Starting nzx50_financials_process_db - download_nzx50_S3()')
        filepath_name = "/tmp/%s/%s/%s" % (S3_BUCKNAME, LOOKUP_PATH, STLIST_FNAME)
        try:
            filepath = Path(filepath_name)
        except FileNotFoundError:
            print("filename: %s not exists" % filepath_name)
        else:
            print("filename: %s exists" % filepath_name)
            os.remove(filepath_name)
        hook = S3Hook(AWS_S3_CONN_ID)
        local_path = "/tmp/%s/%s/" % (S3_BUCKNAME, LOOKUP_PATH)
        key_filename = "%s/%s" % (LOOKUP_PATH, STLIST_FNAME)
        print ("filename: %s" % key_filename)
        hook.download_file(
            key=key_filename,
            bucket_name=S3_BUCKNAME,
            local_path=local_path,
            preserve_file_name=True,
            use_autogenerated_subdir=False
        )

    @task()
    def insert2_db():
        print ('Starting nzx50_financials_uploads_S3')
        symbols = _get_nzx50_symbols()
        insert_count = 0
        for i, symbol in symbols.iterrows():
            symbol = "%s.NZ" % symbol['Profile']
            jsonv = _get_financials_file(symbol)
            print(json.dumps(jsonv))
            print(" in-progress... %s" % symbol)
            PostgresOperator(
                task_id='process_nzx50_into_db',
                postgres_conn_id='postgres_conn',
                sql="""
                INSERT INTO st.hist_capture (capture_type, capture_run, capture_info, capture_dump)
                VALUES (%(job)s, %(target)s, %(symbol)s, %(json)s)
                """,
                #parameters={ "json" : json.dumps({ "NZDUSD=X" : {} }) }
                parameters={ "job": JOB_NAME, "target": TARGET_PATH, "symbol": symbol, "json": json.dumps(jsonv) }
            ).execute({})
            insert_count+=1
        print("Total Insert: %s!" % str(insert_count))
    
    @task()
    def log_complete2_db():
        print("Starting... tag_S3_complete_nzx50")
        # Establish connection using PostgresHook
        hook = PostgresHook(postgres_conn_id=DB_CONN)
        sql = "insert into st.run_log (log_status, log_type, log_info) values (%(status)s, %(job)s, %(target)s)"
        
        result = hook.run(sql, parameters={ "status": "COMPLETED", "job": JOB_NAME, "target": TARGET_PATH + ":" + STLIST_FNAME })
        
        # Log and return the result
        if result:
            print(f"Previous Run: {result}")
            raise Exception("Already Completed Previously!")
            return result
        else:
            print("No rows found in the table.")
            return None

    check4_previous_run() >> getfrom_S3() >> insert2_db() >> log_complete2_db()

nzx50_financials_process2_db()   